---
title: "Amazon and Times Article Ethics"
format:
  html:
    toc: true
    number-sections: true
    code-fold: true
    theme: cosmo
bibliography: references.bib
citeproc: true
---

With these two articles being written in 2018 and 2020, it is clear that data-driven tools have been adopted in multiple pertinent areas of society like hiring and policing in the name of efficiency, especially since machine learning was at a time of high growth in this time due to “a surge in low-cost computing power”. Unfortunately, creating these tools in the name of efficiency can appear appealing to companies due to the vast amount of computing power that algorithms can complete in a short amount of time. However, at the same time, these same individuals adapting these policies overlook other significant factors, such as amplified bias and real-world harm. Specifically, this is in regards to Reuter’s article from 2018 with Amazon’s secret AI recruiting tool which ended up showing bias against women, as well as a Times article in 2020 where Robert Williams @williams2024wrongful was wrongfully arrested due to a false target by a law enforcement algorithmic tool with a faulty facial recognition match. 

In the Amazon @dastin2018amazon, the company built an AI tool to speed up the hiring process by designating AI to score resumes on a five-star scale, with this eventually ending up penalizing resumes which included words related to “woman”. This is because the model was trained based on Amazon resumes which were submitted over the last 10 years, most of which came from men due to the gender imbalance in tech. This resulted in increased discovery of how data that is biased in training can result in discriminatory decisions with real-world negative impacts; and although this tool was eventually shut down, it raises the ethical concerns of solely relying on AI in hiring decisions. Further, in the Times @williams2024wrongful article, Robert Williams recounts the trauma his family faced after he was dragged out by police officers due to a faulty facial recognition match despite him having no connection to the alleged crime. Williams’ expired driver’s license was matched to blurry security footage, with this being the basis for which law enforcement decided to arrest Williams in front of his family. This is an additional testimony in which sole reliance on poorly-trained algorithms can have extreme negative consequences that are difficult to overturn, directly impacting marginalized communities the most. In both of these cases, the unchecked use of machine learning resulted in the perpetuation of systemic inequities thus increasing the necessity for data science tools created with accountability.

In terms of consent structures for recruiting participants, although the participants may have potentially signed some sneaky Terms and Conditions, the data ultimately was used without informed and clear permissions from the individuals affected. Amazon @dastin2018amazon used prior resumes from job applicants to train their AI model which ended up being biased against women and ultimately disenfranchising their hiring process, without explicitly informing applicants that their data would be used to train a model. In Robert William’s case, the use of his expired driver’s license photo from a state database to identify him without his consent did lack explicit content. Now, this is to say that there’s no way someone who is potentially guilty of a crime would ever agree to an algorithm that would ultimately identify them as a perpetrator to a crime. But, the issue at hand is whether anyone who is guilty or not should be subject to surveillance or judgment by an algorithm without clear and informed consent. This is why consent is based upon the pillars of accountability and transparency, and in both of the contexts of our article there is an absence of how individual data was used resulting in unchecked systemic discrimination.

This leads in to the fact that the data used was in fact identifiable, with the Amazon resume AI model being trained off of real resumes with education history, affiliations, and names, ultimately learning to penalize the achievements of women that the model would come across. Robert William’s situation resulted in his personal facial technology being used to pinpoint him in a case he was not at all complicit in, with his facial features being highly personal biometric data. In the case of Amazon @dastin2018amazon, the data was not anonymized to the extent it should have been since the AI tool ended up learning how to discriminate and infer gender. With Williams, there is no irreversibility to facial recognition, thus highlighting the need for proper oversight and safeguards for personal biometric data.

In the case of who was measured, Amazon @dastin2018amazon trained its models based on past resumes presumably for its engineering-related positions; which unfortunately are roles predominantly occupied by men. Due to this oversight in the training process, the AI model repeatedly seeing resumes by males must have made it assume that these resumes are somehow “superior” and began to associate male candidates as being rated higher. This becomes problematic since Amazon’s mission of hiring a diverse range of candidates was not met here since they likely used resumes majority held by men for their model, with this not being representative of the people we’d like to generalize to the algorithm. This makes it difficult to assess the ethics of analyzing data for which the implications of its use are not fully understood, especially since the lack of this consideration can perpetuate biases they aim to reduce.

In terms of race and gender being used as a variable, while these variables were not explicitly mentioned in the training models, they ended up playing a major role in the outcomes of the algorithms due to false training data. Amazon’s AI tool inferred the resumes of males to be related to higher quality, and the false identification of Williams is testament to the frequency of which facial recognition systems unfortunately misclassify Black individuals. These factors were not acknowledged presumably in the initial training process, but ended up in discriminatory results. These two examples show how identities can be turned into inherent statistical disadvantages that show how race can result in how likely how someone can be misidentified, and gender-related affiliations were proxes for someone’s gender and how worthy of a candidate they were for the resume rating.

@dastin2018amazon

@williams2024wrongful
