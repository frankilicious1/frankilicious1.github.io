---
title: "Amazon and Times Article Ethics"
format:
  html:
    toc: true
    number-sections: true
    code-fold: true
    theme: cosmo
bibliography: references.bib
citeproc: true
---

Data-driven efficiencies are at the forefront of mediums in terms of hiring and policing in order to promote efficiency, which is why companies itch to adopt technologies they believe will save them time and money. Creating these tools in the name of efficiency can appear appealing to companies due to the vast amount of computing power that algorithms can complete in a short amount of time. However, at the same time, these same individuals adapting these policies overlook other significant factors, such as amplified bias and real-world harm. Specifically, the mention of bias and harm is in regards to Reuter’s article from 2018 with Amazon’s secret AI recruiting tool which ended up showing bias against women, as well as a Times article in 2020 where Robert Williams @williams2024wrongful was wrongfully arrested due to a false target by a law enforcement algorithmic tool with a faulty facial recognition match. 

In the Amazon @dastin2018amazon, the company built an AI tool to speed up the hiring process by designating AI to score resumes on a five-star scale, with the tool eventually ending up penalizing resumes which included words related to “woman”. This is because the model was trained based on Amazon resumes which were submitted over the last 10 years, most of which came from men due to the gender imbalance in tech. The model led to an increased discovery of how data which is biased in training can result in discriminatory decisions with real-world negative impacts; and although this tool was eventually shut down, it raises the ethical concerns of solely relying on AI in hiring decisions. Further, in the Times @williams2024wrongful article, Robert Williams recounts the trauma his family faced after he was dragged out by police officers due to a faulty facial recognition match despite him having no connection to the alleged crime. Williams’ expired driver’s license was matched to blurry security footage, with the expired license being the basis for which law enforcement decided to arrest Williams in front of his family. This is an additional testimony in which sole reliance on poorly-trained algorithms can have extreme negative consequences proving difficult to overturn, directly impacting marginalized communities the most. In both of these cases, the unchecked use of machine learning resulted in the perpetuation of systemic inequities thus increasing the necessity for data science tools created with accountability.

In terms of consent structures for recruiting participants, although the participants may have potentially signed some sneaky Terms and Conditions, the data ultimately was used without informed and clear permissions from the individuals affected. Amazon @dastin2018amazon used prior resumes from job applicants to train their AI model which ended up being biased against women and ultimately disenfranchising their hiring process, without explicitly informing applicants their data would be used to train a model. In Robert William’s case, the use of his expired driver’s license photo from a state database to identify him without his consent did lack explicit content. Now, this is to say there’s no way someone who is potentially guilty of a crime would ever agree to an algorithm which would ultimately identify them as a perpetrator to a crime. But, the issue at hand is whether anyone who is guilty or not should be subject to surveillance or judgment by an algorithm without clear and informed consent. From the inquiry, it is evident why consent is based upon the pillars of accountability and transparency. In both of the contexts of our article, there is an absence of how individual data was used resulting in unchecked systemic discrimination, leading to concerns about unregulated data.

Amazon used real resumes with real demographic information; and while this data may not have been public, the model still inferred gender raising concerns about implicit bias in training models. The issue with Robert Williams' case explicitly is the use of unique features that are seemingly secure without explicit consent. Facial recognition technology matched a blurry driver's license DMV photo from a government database, without his knowledge and consent. Most training images are seemingly volunteered by individuals who never explicitly consent to this use, with this being prevalent in camera footage and DMV photos. In the case of Amazon @dastin2018amazon, the data was not anonymized to the extent it should have been since the AI tool ended up learning how to discriminate and infer gender. With Williams, there is no irreversibility to facial recognition, thus highlighting the need for proper oversight and safeguards for personal biometric data.

In the case of who was measured, Amazon @dastin2018amazon trained its models based on past resumes presumably for its engineering-related positions; which unfortunately are roles predominantly occupied by men. Due to this oversight in the training process, the AI model repeatedly seeing resumes by males must have made it assume that these resumes are somehow “superior” and began to associate male candidates as being rated higher. This becomes problematic since Amazon’s mission of hiring a diverse range of candidates was not met here. Because Amazon used resumes from a ten-year span in a period where most employees hired were men, male resumes may have been overvalued as successful leading to bias in the training model.

In terms of race and gender being used as a variable, while these variables were not explicitly mentioned in the training models, they ended up playing a major role in the outcomes of the algorithms due to false training data. Amazon’s AI tool inferred the resumes of males to be related to higher quality, and the false identification of Williams is testament to the frequency of which facial recognition systems unfortunately misclassify Black individuals. These factors were not acknowledged presumably in the initial training process, but ended up in discriminatory results. These two examples show how identities can be turned into inherent statistical disadvantages that show how race can result in how likely how someone can be misidentified, and gender-related affiliations were proxes for someone’s gender and how worthy of a candidate they were for the resume rating.

@dastin2018amazon

@williams2024wrongful
