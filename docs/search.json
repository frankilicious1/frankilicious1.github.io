[
  {
    "objectID": "objectsinspace.html",
    "href": "objectsinspace.html",
    "title": "Annual Number of Objects Launched In Outer Space",
    "section": "",
    "text": "Here, my graph shows the number of objects launched into space from 1957 until present day. Credit goes to Our World in Data and the United Nations Office for Outer Space Affairs for collecting national registers of launches submitted to the UN.\n\n\n\n\n\n\n\n\n\nUnited Nations Office for Outer Space Affairs. (2025). Cumulative number of objects launched into space – UNOOSA [Data set]. Our World in Data. https://ourworldindata.org/grapher/cumulative-number-of-objects-launched-into-outer-spaceResearchGate+6Our World in Data+6Our World in Data+6"
  },
  {
    "objectID": "trafficsql.html",
    "href": "trafficsql.html",
    "title": "Traffic Data SQL",
    "section": "",
    "text": "Introduction\nThis SQL wrangling analysis focuses on the Stanford Open Policing Project Pierson et al. (2020) which standardizes data on vehicle and pedestrian spots from law enforcement across the country in order to combine statistical analysis and data journalism. My project focuses on the data from Charlotte, Chicago, and New Orleans, with this data coming from the Stanford Open Policing Project published by Pierson et al. (2020). Pierson et al. (2020) The data is measured post-pandemic, which is good context since this closely follows the George Floyd movement. The population of the cities vary since Charlotte has 900,000 residents, Chicago 2.6 million, and New Orleans 364,000. While this may seem unusual to analyze cities in such different populations, I wanted to have cities of size small, medium, large so that the metric used of search rate per stop controls for total stop volume.\nThis bar chart compares the search rate by age group across Charlotte, Chicago, and New Orleans.\n\n\n\n\n\n\n\n\n\nNew Orleans stands out as having the highest search rates among all age demographics, especially for drivers under 30 with the rate being around 0.2 for minors and just above 0.15 for adults under 30. Charlotte has search rates that are a bit more moderate and declining with age, while Chicago’s search rates remain much lower regardless of age. Some reasons for New Orleans have such high search rates may be due to reports of the city over policing its communities. Even though the height of this controversy was in 2013, the remnants of this may still remain. Chicago may have placed more reforms for its policing efforts, especially since its 2014 case with Laquan McDonald sparked controversy in communities. Chicago may have shifted their policing more towards data-driven policing or citation based enforcement so there aren’t as many physical searches. In addition, training methods in police departments across the U.S. may vary extensively which can account for Chicago’s low search rates regardless of age group.\n\n\n\n\n{#fig-alt:“GroupedbarchartcomparingnowthearrestratesbyagegroupacrossCharlotte,Chicago,andNewOrleans.NewOrleansshowsthehighestarrestrateforminors,whileChicagohasatadhigharrestratesforadultsaged18to45” width=672}\n\n\n\n\nThe arrest rates tell quite a story here across the cities of Charlotte, Chicago, and New Orleans. New Orleans has a massively higher arrest rate for minors with it being at almost 28%. Chicago has high arrests for adults 18-45 with them being almost at 20%. Despite Chicago having low search rates, stops likely escalate to arrests. Charlotte consistently across age groups has lower arrest rates, with this indicating extreme variance in how these cities approach policing. The 28% arrest rate for minors in New Orleans is a bit alarming because of overcriminalization in youth and how this may shape their future having a record. This is why understanding the factors of policing is critical to ensuring healthy communities that are functional.\n\n\n\n5 records\n\n\nage_group\ntotal_stops\nsearch_rate\narrest_rate\n\n\n\n\n&lt;18\n22252\n0.0986\n0.0436\n\n\n18–30\n736017\n0.0799\n0.0426\n\n\n31–45\n562977\n0.0441\n0.0306\n\n\n46–60\n228302\n0.0288\n0.0208\n\n\n60+\n48898\n0.0096\n0.0068\n\n\n\n\n\nIn Charlotte’s situation, search rates and arrest rates decline with wage. These rates are in the medium ratio compared to Chicago and New Orleans. There is an arrest rate that is highest with minors and younger adults, lining up with other relationships. There is a an age-related disparity in enforcement, especially since senior citizens have less than 1% of search and arrest rates based on total stops.\n\n\n\n5 records\n\n\nage_group\ntotal_stops\nsearch_rate\narrest_rate\n\n\n\n\n&lt;18\n7970\n0.0153\n0.0033\n\n\n18–30\n609351\n0.0122\n0.1762\n\n\n31–45\n502562\n0.0071\n0.1505\n\n\n46–60\n259737\n0.0035\n0.0996\n\n\n60+\n91455\n0.0018\n0.0361\n\n\n\n\n\nChicago has the lowest search rates with them never exceeding 1.53%. However, something that is interesting to note here is that even though 1% of individuals aged 18-30 are searched, over 17% are arrested. This may be influenced by stops being more warranted in Chicago, meaning they may be due to warrants or other severe violations that are not related to trivial stops like speeding.\n\n\n\n5 records\n\n\nage_group\ntotal_stops\narrest_rate\nsearch_rate\n\n\n\n\n&lt;18\n18487\n0.2801\n0.2106\n\n\n18–30\n220256\n0.1926\n0.1612\n\n\n31–45\n150491\n0.1930\n0.1543\n\n\n46–60\n90071\n0.1656\n0.1253\n\n\n60+\n20001\n0.0906\n0.0613\n\n\n\n\n\nNew Orleans by far has the highest arrest rates, with minors being arrested in 28% of total stops. Search rates are also quite high for young drivers with the rate being at 21%. There may be even less of an age-related disparity in New Orleans since their arrest rates for individuals 60+ are 9% and search rates 6%, possibly suggesting a much stricter enforcement-based approach irrespective of age. This heavy policing in youth may be potentially related to to a lack of policies helping teenagers with their time like cocurricular and extracurricular programming funded by the city. In addition, law enforcement could potentially bias younger individuals as more likely to be reckless, associating them with trouble.\nOverall, our data shows that youth under 30 are under high scrutiny by police, especially in New Orleans with its high search and arrest rates. In Chicago, the data is quite interesting since it presents rare searches but high arrests. This may reflect a difference in philosophy of the respective cities policing. The consequences are quite real especially since youth in New Orleans are arrested in nearly one-third of stops, a reality that may shape their access to education and employment for years. Understanding policing trends and pairing them with research is essential to safeguard communities.\n\n\n\n\n\n\nReferences\n\nPierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker, Vignesh Ramachandran, and Sharad Goel. 2020. “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour, 1–10."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi there, my name is Frank and I am currently a sophomore at Pomona College! I was raised in Chicago but originally was born in Poland. I enjoy hiking, visiting new places, biking, and finding new TV shows to binge. Currently, my favorite show to watch is Sex and the City!"
  },
  {
    "objectID": "ethics.html",
    "href": "ethics.html",
    "title": "Amazon and Times Article Ethics",
    "section": "",
    "text": "Data-driven efficiencies are at the forefront of mediums in terms of hiring and policing in order to promote efficiency, which is why companies itch to adopt technologies they believe will save them time and money. Creating these tools in the name of efficiency can appear appealing to companies due to the vast amount of computing power that algorithms can complete in a short amount of time. However, at the same time, these same individuals adapting these policies overlook other significant factors, such as amplified bias and real-world harm. Specifically, the mention of bias and harm is in regards to Reuter’s article from 2018 with Amazon’s secret AI recruiting tool which ended up showing bias against women, as well as a Times article in 2020 where Robert Williams Williams (2024) was wrongfully arrested due to a false target by a law enforcement algorithmic tool with a faulty facial recognition match. \nIn the Amazon Dastin (2018), the company built an AI tool to speed up the hiring process by designating AI to score resumes on a five-star scale, with the tool eventually ending up penalizing resumes which included words related to “woman”. This is because the model was trained based on Amazon resumes which were submitted over the last 10 years, most of which came from men due to the gender imbalance in tech. The model led to an increased discovery of how data which is biased in training can result in discriminatory decisions with real-world negative impacts; and although this tool was eventually shut down, it raises the ethical concerns of solely relying on AI in hiring decisions. Further, in the Times Williams (2024) article, Robert Williams recounts the trauma his family faced after he was dragged out by police officers due to a faulty facial recognition match despite him having no connection to the alleged crime. Williams’ expired driver’s license was matched to blurry security footage, with the expired license being the basis for which law enforcement decided to arrest Williams in front of his family. This is an additional testimony in which sole reliance on poorly-trained algorithms can have extreme negative consequences proving difficult to overturn, directly impacting marginalized communities the most. In both of these cases, the unchecked use of machine learning resulted in the perpetuation of systemic inequities thus increasing the necessity for data science tools created with accountability.\nIn terms of consent structures for recruiting participants, although the participants may have potentially signed some sneaky Terms and Conditions, the data ultimately was used without informed and clear permissions from the individuals affected. Amazon Dastin (2018) used prior resumes from job applicants to train their AI model which ended up being biased against women and ultimately disenfranchising their hiring process, without explicitly informing applicants their data would be used to train a model. In Robert William’s case, the use of his expired driver’s license photo from a state database to identify him without his consent did lack explicit content. Now, this is to say there’s no way someone who is potentially guilty of a crime would ever agree to an algorithm which would ultimately identify them as a perpetrator to a crime. But, the issue at hand is whether anyone who is guilty or not should be subject to surveillance or judgment by an algorithm without clear and informed consent. From the inquiry, it is evident why consent is based upon the pillars of accountability and transparency. In both of the contexts of our article, there is an absence of how individual data was used resulting in unchecked systemic discrimination, leading to concerns about unregulated data.\nAmazon used real resumes with real demographic information; and while this data may not have been public, the model still inferred gender raising concerns about implicit bias in training models. The issue with Robert Williams’ case explicitly is the use of unique features that are seemingly secure without explicit consent. Facial recognition technology matched a blurry driver’s license DMV photo from a government database, without his knowledge and consent. Most training images are seemingly volunteered by individuals who never explicitly consent to this use, with this being prevalent in camera footage and DMV photos. In the case of Amazon Dastin (2018), the data was not anonymized to the extent it should have been since the AI tool ended up learning how to discriminate and infer gender. With Williams, there is no irreversibility to facial recognition, thus highlighting the need for proper oversight and safeguards for personal biometric data.\nIn the case of who was measured, Amazon Dastin (2018) trained its models based on past resumes presumably for its engineering-related positions; which unfortunately are roles predominantly occupied by men. Due to this oversight in the training process, the AI model repeatedly seeing resumes by males must have made it assume that these resumes are somehow “superior” and began to associate male candidates as being rated higher. This becomes problematic since Amazon’s mission of hiring a diverse range of candidates was not met here. Because Amazon used resumes from a ten-year span in a period where most employees hired were men, male resumes may have been overvalued as successful leading to bias in the training model.\nIn terms of race and gender being used as a variable, while these variables were not explicitly mentioned in the training models, they ended up playing a major role in the outcomes of the algorithms due to false training data. Amazon’s AI tool inferred the resumes of males to be related to higher quality, and the false identification of Williams is testament to the frequency of which facial recognition systems unfortunately misclassify Black individuals. These factors were not acknowledged presumably in the initial training process, but ended up in discriminatory results. These two examples show how identities can be turned into inherent statistical disadvantages that show how race can result in how likely how someone can be misidentified, and gender-related affiliations were proxes for someone’s gender and how worthy of a candidate they were for the resume rating.\nDastin (2018)\nWilliams (2024)\n\n\n\n\nReferences\n\nDastin, Jeffrey. 2018. “Amazon Scraps Secret AI Recruiting Tool That Showed Bias Against Women.” 2018. https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G.\n\n\nWilliams, Robert. 2024. “I Was Wrongfully Arrested Because of Facial Recognition Technology. It Shouldn’t Happen to Anyone Else.” 2024. https://time.com/6991818/wrongfully-arrested-facial-recognition-technology-essay/."
  },
  {
    "objectID": "theoffice.html",
    "href": "theoffice.html",
    "title": "The Office!",
    "section": "",
    "text": "The source of this code is from the Kaggle website showcasing every quote from every season of The Office, acquired by https://www.kaggle.com/datasets/fabriziocominetti/the-office-lines.\nThis first line of code visualizes what word comes up the most after a character in The Office says ‘hey’! To create the first graph, I filtered each line for the word “hey” and extracted the word immediately following it using a regular expression. I then grouped those words to count how often each came after “hey” and visualized the top 10. It makes sense that after hey the most common word said is guys, probably Michael coming in to annoy everyone in the workplace!\n\n\n\n\n\n\n\n\n\nI thought I was on to something with this idea, thinking there would be some clear distinctions with this question. For this graph, I calculated the number of characters in each line and then averaged those values by character name. . The character unsurprisingly with the longest character lines is Michael, which makes sense since he would have a lot of funny monologues that contributed a lot to his development as a character and a manager. David Brent is in the UK version of The Office which I had to look up, but would also make sense that he is the second character. The other characters have more on par average line lengths, so this seems like the longer the character lines are is due to some sort of promotional / special event occurring in the episode rather than regular dialogue.\n\n\n\n\n\n\n\n\n\nThis one I could have easily predicted! I feel like I can always hear Michael’s voice in the back of my head, answering some sort of questions. This chart is based on detecting whether a line ends with a question mark. I used a regular expression to determine the lines and then grouped them by the character, and counted the number of questions by each character. Jim comes to a near second, which is not something I expected in this dataset.\n\n\n\n\n\n\n\n\n\nHere, I used a regular expression to detect lines ending in an exclamation mark. Michael again here yells so much! It seems like he may be the main character after all… Lol\nBut Dwight comes to a close second. This is really interesting to me that they come so close, but they did clash with a lot of characters.\n\n\n\n\n\n\n\n\n\nThanks for reading prof!"
  },
  {
    "objectID": "postoffice.html",
    "href": "postoffice.html",
    "title": "Post Office Development Permutation Test",
    "section": "",
    "text": "The data for this project of post offices that operated in the United States between 1639-2000 comes from Cameron Blevins and Richard W. Helbock in the Harvard Dataverse. For this project, I chose the first option which is conducting a permutation test by simulator behavior under the null hypothesis. Specifically, I sought to see whether the average number of post offices established on a per decade basis has decreased in recent years, specifically before 1900 compared to after 1900 until 2000.\nThis dataset provides context for how many post offices were implemented yearly since the birth of the U.S. I mutated another variable into established decade in order to group the amount of post offices built into a decade basis for easier visualization. Then, this data is grouped into an early time period which is before 1900 and then after 1900 which would be considered modern time.\nAcknowledging the critical role post offices played in the development of the U.S., it is fascinating to visualize and interpret this data to see how the priority of post offices for vital communication and shipping may have changed in the past centuries. This is especially so since this data allows visualization of U.S. infrastructure growth in recent years as well as U.S. westward expansion.\nIn the plot above, we see the number of post offices built from 1800 to 2000. There is a clear surge in the 1800s with this number eventually dwindling down, fueling the hypothesis for this project since there may have been more of an emphasis in post offices in the 1800s.\nIn this histogram running our permutation test, we observe the null distribution of the difference in average post offices per decade between early and modern periods. This assumes “early” and “modern” have no real effect under the null hypothesis. Each bar is one of 1,000 simulated differences after the labels had been randomly shuffled.\nThe red dashed value is our observed value is quite far to the right, suggesting that a drastic difference is not likely just because of chance. This provides evidence for our hypothesis that the early period of the U.S. before 1900 had much more post office development than the modern period. This makes sense since U.S. historical expansion was quite concentrated in the 1800s as westward expansion was in its prime as well as the U.S. government attempting to spread out its infrastructure. mean(null_diffs &gt;= obs_diff)\nThe p-value being 0.005 means only 0.5% of the permuted differences were as large as the observed ones, rejecting the null hypothesis. This supports that the pre-1900s saw more growth in post offices than the modern period. This is an interesting glimpse into the early development of the United States and how the value of post offices may have shifted over the history of our country as well our priorities as a society.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[1] 0.005\n\n\nOriginally, this is a TidyTuesday analysis by jonthegeek. This data comes form Cameron Blevins and Richard W. Helbock. They have 166,140 post offices in the United States between 1639 and 2000. Through this, we see a year-by-year snapshot of the national postal system over several decades, making this an extremely expansive data set.\n“Blevins, Cameron; Helbock, Richard W., 2021,”US Post Offices”, https://doi.org/10.7910/DVN/NUKCNA, Harvard Dataverse, V1, UNF:6:8ROmiI5/4qA8jHrt62PpyA== [fileUNF]”"
  },
  {
    "objectID": "simpsons.html",
    "href": "simpsons.html",
    "title": "Simpsons Average Episode Rating",
    "section": "",
    "text": "My graph shows the average IMDb rating of Simpsons episodes from 2010-2016 from the Tidyverse set made on 2025-02-04 with credit to Prashant Banerjee and Kaggle for making this data available to the public.\nThis data is taken from a Tidy Tuesday Github repository which explores a Kaggle Simpsons dataset of 600 Simpsons episodes. Further, the Kaggle dataset rehosted the original data scrape which occurred in 2016 with credit to Todd Schneider.. A lot of hierarchy to data, I know.\n\n\n\n\n\n\n\n\n\nBanerjee, P. (n.d.). The Simpsons dataset. Kaggle. https://www.kaggle.com/datasets/prashant111/the-simpsons-dataset\nData Science Learning Community. (2025, February 4). Donuts, data, and D’oh – A deep dive into The Simpsons [Data set]. GitHub. https://github.com/rfordatascience/tidytuesday/blob/main/data/2025/2025-02-04/readme.md\nSchneider, T. W. (2016, September 28). The Simpsons by the data. Todd W. Schneider. https://toddwschneider.com/posts/the-simpsons-by-the-data/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Frank’s Website",
    "section": "",
    "text": "Github Repo: https://github.com/frankilicious1/frankilicious1.github.io.git"
  },
  {
    "objectID": "finalpres.html#project-1-objects-launched-into-space",
    "href": "finalpres.html#project-1-objects-launched-into-space",
    "title": "Final Presentation",
    "section": "Project 1 – Objects Launched Into Space",
    "text": "Project 1 – Objects Launched Into Space\n\nOverview\n\n\n\nNumber of objects launched into space from 1960 until present day today\n\n\n\n\n\nCode Visualization\n\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\n\nouter_space_objects &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-04-23/outer_space_objects.csv')\n\nlaunches_per_year &lt;- outer_space_objects %&gt;%\n  group_by(Year) %&gt;%\n  summarize(total_objects = sum(num_objects, na.rm = TRUE)) %&gt;%\n  arrange(Year)\n\nmax_launch_year &lt;- launches_per_year %&gt;%\n  filter(total_objects == max(total_objects))\n\nggplot(launches_per_year, aes(x = Year, y = total_objects)) +\n  geom_line(color = \"blue\") +\n  geom_point(alpha = 0.2) +\n  geom_text(aes(label = ifelse(total_objects == max(total_objects), \n                               paste0(total_objects, \" launches\"), \"\"))) +\n  labs(title = \"Trend of Space Object Launches by Year\",\n       x = \"Year\",\n       y = \"Number of Objects Launched\") +\n  theme_minimal()"
  },
  {
    "objectID": "finalpres.html#project-5-traffic-stops-by-age-group",
    "href": "finalpres.html#project-5-traffic-stops-by-age-group",
    "title": "Final Presentation",
    "section": "Project 5 – Traffic Stops by Age Group",
    "text": "Project 5 – Traffic Stops by Age Group\nOverview\n\nUsing the Stanford Open Policing Project\n\nComparing Charlotte, Chicago, and New Orleans\n\nFocused on age-based disparities in search and arrest rates\n\nSearch Rate by Age\n\nNew Orleans has the highest search rates across all age groups\nYoung drivers (under 30) face the most frequent searches\nCharlotte has moderate search rates that decline with age\nChicago has consistently low search rates regardless of age\n\n\nlibrary(ggplot2)\n\nggplot(age_data, aes(x = age_group, y = search_rate, fill = city)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Search Rate by Age Group\",\n    x = \"Age Group\",\n    y = \"Search Rate\",\n    fill = \"City\"\n  )\n\n\n\n\n\n\n\n\nArrest Rate by Age\n\nMinors in New Orleans are arrested in nearly 28% of stops\nChicago shows high arrest rates for ages 18–45 despite low search rates\nArrest trends reveal major differences in how cities enforce stops by age\n\n\nggplot(arrest_data, aes(x = age_group, y = arrest_rate, fill = city)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Arrest Rate by Age Group\",\n    x = \"Age Group\",\n    y = \"Arrest Rate\",\n    fill = \"City\"\n  )"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "test test test"
  }
]